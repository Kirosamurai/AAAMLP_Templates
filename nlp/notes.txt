notes cuz my notebook was thrown out

To appraoch NLP problems,

1. For sentiment analysis, simple method would be to create a 
list of good and bad words and count their occurences in the document
and give a +ve/-ve sentiment depending on their counts.

2. Simplest way, use a tokenizer to split it. CountVectorizer creates a sparse
matrix of that document and counts how many times each word is used in the text.
This creates a sparse matrix with the words as features, then we can use a model
to predict the labels.

3. Another method is the TF-IDF method.

TF = (number of times a term t apprears)/(Total number of terms in the document)
IDF = log(total number of documents/number of documents with term t in it)

tf-idf = tf*idf

this creates a sparse vector with tf-idf value for each word.
(new benchmark now)

4. n-grams: combinations of n words in order. ex-
sentence = "hi, how are you?" into 3-grams give
[('hi', ',', 'how'), 
 (',', 'how', 'are'), 
 ('how', 'are', 'you'), 
 ('are', 'you', '?')] 

this adds a bit of context into the data.
** we can use this along with the above 2 methods to further
icnrease the accuracy.

5. stemming and lemmatization: "reducing the infected words to the root word"
stemming gives the basic form of the root word, i.e. it may or may not make sense,
lemmatization gives the root word which actually makes sense and is in a dictionary.

adding this can also help improve the model, gives a bit more context.

6. topic extraction: singular value decomposition, reduces the data into a given number
of components. Can use this on the matrix obtained from CountVectorizer or TfidfVectorizer

7. stopwords: not the best idea to remove them since we can incur a lot of information loss.
ex- "i need a new dog" convert to "need new dog", who needs a new dog? we dk.

DEEP LEARNING:

word embeddings: words embeddings are vectors used to represent words. Google's word2vec, 
FastText from facebook, GloVe from Stanford are all methods to create word embeddings.

The basic idea is to build a shallow network that learns the embeddings for words by reconstruction
of an input sentence. So, we can train a network to predict the missing word by using the surrounding 
words and it will update the embeddings as it moves forward. This apporach is known as the
Continuous Bag of Words or CBoW model. 

We can also take one word as use it to predict the context words. Word2Vec uses this method.

FastText learns embeddings for character n-grams instead.

visualization: word embeddings can be viewed as vectors with linguistic context, for ex-
germany - berlin + paris = france (removing the capital from a country and adding paris gives that country)
This  is  not  always  true,  but  examples  like  these  are  useful  for 
understanding the usefulness of word embeddings

(king + woman = queen)

sentences: adding embeddings for words can give embeddings for sentences.
ex- hi, how are you? gives a sentence embedding after normalization.

8. note that we can imagine a text data as a time series data, with each word 
being a record in a continuous time, thus, we can use models primarily for time
series data such as CNNs, GRUs, LSTMs etc. for them as well.


